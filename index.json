
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"","date":1718755200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1718755200,"objectID":"890de7da2d85d86bcb2cc68256a0375e","permalink":"https://BUPT-PRIS-727.github.io/authors/chuang-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/chuang-zhang/","section":"authors","summary":"","tags":null,"title":"Chuang Zhang","type":"authors"},{"authors":null,"categories":null,"content":"","date":1718755200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1718755200,"objectID":"e829e7859b84bc39e6561775d1cd5f02","permalink":"https://BUPT-PRIS-727.github.io/authors/kaixin-chen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/kaixin-chen/","section":"authors","summary":"","tags":null,"title":"Kaixin Chen","type":"authors"},{"authors":null,"categories":null,"content":"","date":1718755200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1718755200,"objectID":"e4888d1c039497bf12ea5cdc8d23c019","permalink":"https://BUPT-PRIS-727.github.io/authors/mengqiu-xu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/mengqiu-xu/","section":"authors","summary":"","tags":null,"title":"Mengqiu Xu","type":"authors"},{"authors":null,"categories":null,"content":"","date":1718755200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1718755200,"objectID":"9b9363c6226705e57e6840dbc5879ee6","permalink":"https://BUPT-PRIS-727.github.io/authors/ming-wu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/ming-wu/","section":"authors","summary":"","tags":null,"title":"Ming Wu","type":"authors"},{"authors":null,"categories":null,"content":"","date":1718755200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1718755200,"objectID":"e3c740f33c31779c02884904ea4a80b2","permalink":"https://BUPT-PRIS-727.github.io/authors/mingrui-xu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/mingrui-xu/","section":"authors","summary":"","tags":null,"title":"Mingrui Xu","type":"authors"},{"authors":null,"categories":null,"content":"","date":1718755200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1718755200,"objectID":"f7c0ab6ead3874254f222a0ab35e46e2","permalink":"https://BUPT-PRIS-727.github.io/authors/yiqing-feng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yiqing-feng/","section":"authors","summary":"","tags":null,"title":"Yiqing Feng","type":"authors"},{"authors":null,"categories":null,"content":"","date":1718755200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1718755200,"objectID":"5b8bc4d49b4e279fd3253e37a988ab3a","permalink":"https://BUPT-PRIS-727.github.io/authors/yixiang-huang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yixiang-huang/","section":"authors","summary":"","tags":null,"title":"Yixiang Huang","type":"authors"},{"authors":null,"categories":null,"content":"","date":1701993600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1701993600,"objectID":"5b2b86a9bd1d345bd36b999303ff7481","permalink":"https://BUPT-PRIS-727.github.io/authors/xun-zhu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/xun-zhu/","section":"authors","summary":"","tags":null,"title":"Xun Zhu","type":"authors"},{"authors":null,"categories":null,"content":"","date":1701993600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1701993600,"objectID":"c8e079184dbdf82baf7a02a00862d213","permalink":"https://BUPT-PRIS-727.github.io/authors/ziheng-yang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/ziheng-yang/","section":"authors","summary":"","tags":null,"title":"Ziheng Yang","type":"authors"},{"authors":null,"categories":null,"content":"","date":1696982400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1696982400,"objectID":"822c192028c5791581437b171a09e7da","permalink":"https://BUPT-PRIS-727.github.io/authors/jiaao-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jiaao-li/","section":"authors","summary":"","tags":null,"title":"Jiaao Li","type":"authors"},{"authors":null,"categories":null,"content":"","date":1692576000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1692576000,"objectID":"a048f8fa5474c33127e84a88518eb118","permalink":"https://BUPT-PRIS-727.github.io/authors/haotian-yan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/haotian-yan/","section":"authors","summary":"","tags":null,"title":"Haotian Yan","type":"authors"},{"authors":null,"categories":null,"content":"","date":1692576000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1692576000,"objectID":"8f93af1726893dfc24b779b6008146ad","permalink":"https://BUPT-PRIS-727.github.io/authors/sundingkai-su/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/sundingkai-su/","section":"authors","summary":"","tags":null,"title":"Sundingkai Su","type":"authors"},{"authors":null,"categories":null,"content":"","date":1692576000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1692576000,"objectID":"db93c2f08660b61d9d6d4a7518c91af2","permalink":"https://BUPT-PRIS-727.github.io/authors/yihao-zuo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yihao-zuo/","section":"authors","summary":"","tags":null,"title":"Yihao Zuo","type":"authors"},{"authors":null,"categories":null,"content":"","date":1676937600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1676937600,"objectID":"2367703eb4402cd22454b6f09de4a7a6","permalink":"https://BUPT-PRIS-727.github.io/authors/yutong-xiong/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yutong-xiong/","section":"authors","summary":"","tags":null,"title":"Yutong Xiong","type":"authors"},{"authors":null,"categories":null,"content":"","date":1660262400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1660262400,"objectID":"6db3d5bf80812f3997dee11992c2aefd","permalink":"https://BUPT-PRIS-727.github.io/authors/luming-xiao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/luming-xiao/","section":"authors","summary":"","tags":null,"title":"Luming Xiao","type":"authors"},{"authors":null,"categories":null,"content":"","date":1640908800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1640908800,"objectID":"ae1bdb472c7765b6466627a1cfe21380","permalink":"https://BUPT-PRIS-727.github.io/authors/cheng-lv/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/cheng-lv/","section":"authors","summary":"","tags":null,"title":"Cheng Lv","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"6885266821bef1c93b44064c7736093c","permalink":"https://BUPT-PRIS-727.github.io/authors/bingyao-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/bingyao-li/","section":"authors","summary":"","tags":null,"title":"Bingyao Li","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"8730eb05528d4ee5f369d116b9c0e141","permalink":"https://BUPT-PRIS-727.github.io/authors/bo-yao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/bo-yao/","section":"authors","summary":"","tags":null,"title":"Bo Yao","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"771202abaf93890a876fbdc75c609e74","permalink":"https://BUPT-PRIS-727.github.io/authors/boyuan-jin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/boyuan-jin/","section":"authors","summary":"","tags":null,"title":"Boyuan Jin","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"aafcd2b0225dc19448fd194523bbfcf5","permalink":"https://BUPT-PRIS-727.github.io/authors/di-wu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/di-wu/","section":"authors","summary":"","tags":null,"title":"Di Wu","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"a14d62fcd5a43abed42108c71ef81a01","permalink":"https://BUPT-PRIS-727.github.io/authors/fanbin-mo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/fanbin-mo/","section":"authors","summary":"","tags":null,"title":"Fanbin Mo","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"bde8f9a7e97c320b1ceed4fdecd357fb","permalink":"https://BUPT-PRIS-727.github.io/authors/hailong-guo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/hailong-guo/","section":"authors","summary":"","tags":null,"title":"Hailong Guo","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"e751cde208696d763825a05ee80d3f0d","permalink":"https://BUPT-PRIS-727.github.io/authors/haizhao-sun/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/haizhao-sun/","section":"authors","summary":"","tags":null,"title":"Haizhao Sun","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"bbe1d7879136b10f4cbf22843ae91a2e","permalink":"https://BUPT-PRIS-727.github.io/authors/huiying-chang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/huiying-chang/","section":"authors","summary":"","tags":null,"title":"Huiying Chang","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"1ce4ea1eb27e3bf4a2175ad20a588eed","permalink":"https://BUPT-PRIS-727.github.io/authors/jiahui-xu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jiahui-xu/","section":"authors","summary":"","tags":null,"title":"Jiahui Xu","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f0443faf198b929a51494cdab2f62e11","permalink":"https://BUPT-PRIS-727.github.io/authors/jiaxin-chen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jiaxin-chen/","section":"authors","summary":"","tags":null,"title":"Jiaxin Chen","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"e45b7b57a09c76fa0998f8d7cf3e6931","permalink":"https://BUPT-PRIS-727.github.io/authors/lixuan-du/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/lixuan-du/","section":"authors","summary":"","tags":null,"title":"Lixuan Du","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"0cf00645c7d590cd41f4c193fcf5cc32","permalink":"https://BUPT-PRIS-727.github.io/authors/ran-xu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/ran-xu/","section":"authors","summary":"","tags":null,"title":"Ran Xu","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"42e4dfb126451ef336f3131f38d72461","permalink":"https://BUPT-PRIS-727.github.io/authors/ruhao-xia/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/ruhao-xia/","section":"authors","summary":"","tags":null,"title":"Ruhao Xia","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"81e756cc2bd7e1d8d74e06bf0c735c68","permalink":"https://BUPT-PRIS-727.github.io/authors/ruizhe-ou/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/ruizhe-ou/","section":"authors","summary":"","tags":null,"title":"Ruizhe Ou","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"c465fc7fbd6e8af7583ccea1d2d635bd","permalink":"https://BUPT-PRIS-727.github.io/authors/shenwei-xie/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/shenwei-xie/","section":"authors","summary":"","tags":null,"title":"Shenwei Xie","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"9e4f37eae80b7036984e1a84ba294600","permalink":"https://BUPT-PRIS-727.github.io/authors/sian-xie/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/sian-xie/","section":"authors","summary":"","tags":null,"title":"Sian Xie","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"b546cc486f4b42e827779656e5949792","permalink":"https://BUPT-PRIS-727.github.io/authors/weiqing-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/weiqing-li/","section":"authors","summary":"","tags":null,"title":"Weiqing Li","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"da21e522c31f96dfb9db7a331d1336c2","permalink":"https://BUPT-PRIS-727.github.io/authors/xiao-deng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/xiao-deng/","section":"authors","summary":"","tags":null,"title":"Xiao Deng","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"07a2e017d3b6c9e8c3d055af5b9f6942","permalink":"https://BUPT-PRIS-727.github.io/authors/xing-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/xing-zhang/","section":"authors","summary":"","tags":null,"title":"Xing Zhang","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"9da025946c38b8355b89e32189da3bba","permalink":"https://BUPT-PRIS-727.github.io/authors/xu-ji/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/xu-ji/","section":"authors","summary":"","tags":null,"title":"Xu Ji","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"da80017351a9ee250ffdfd4a5864c2dd","permalink":"https://BUPT-PRIS-727.github.io/authors/yi-zhong/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yi-zhong/","section":"authors","summary":"","tags":null,"title":"Yi Zhong","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"6ee337ccf45dc076a377ec672d885371","permalink":"https://BUPT-PRIS-727.github.io/authors/yifeng-tao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yifeng-tao/","section":"authors","summary":"","tags":null,"title":"Yifeng Tao","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"c64277cd1b920414381b98a16d439fb2","permalink":"https://BUPT-PRIS-727.github.io/authors/yu-gong/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yu-gong/","section":"authors","summary":"","tags":null,"title":"Yu Gong","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f894fcea9c2db0ad9fe367a7664a6ba8","permalink":"https://BUPT-PRIS-727.github.io/authors/yu-ning/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yu-ning/","section":"authors","summary":"","tags":null,"title":"Yu Ning","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"cf6b7def49acf62aabbde32696ea7971","permalink":"https://BUPT-PRIS-727.github.io/authors/yucheng-song/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yucheng-song/","section":"authors","summary":"","tags":null,"title":"Yucheng Song","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"fb72b47c7934b5a47f9f70a1c2ff2b66","permalink":"https://BUPT-PRIS-727.github.io/authors/zhenglin-xian/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/zhenglin-xian/","section":"authors","summary":"","tags":null,"title":"Zhenglin Xian","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"1bf8637398b88387630981cdf87c2ae7","permalink":"https://BUPT-PRIS-727.github.io/authors/zhengrui-chen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/zhengrui-chen/","section":"authors","summary":"","tags":null,"title":"Zhengrui Chen","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Hugo Blox Builder’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://BUPT-PRIS-727.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Hugo Blox Builder's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Mengqiu Xu","Ming Wu","Kaixin Chen","Yixiang Huang","Mingrui Xu","Yujia Yang","Yiqing Feng","Yiying Guo","Bin Huang","Dongliang Chang","Zhenwei Shi","Chuang Zhang","Zhanyu Ma","Jun Guo"],"categories":null,"content":"Abstract Marine fog poses a significant hazard to global shipping, necessitating effective detection and forecasting to reduce economic losses. In recent years, several machine learning (ML) methods have demonstrated superior detection accuracy compared to traditional meteorological methods. However, most of these works are developed on proprietary datasets, and the few publicly accessible datasets are often limited to simplistic toy scenarios for research purposes. To advance the field, we have collected nearly a decade’s worth of multi-modal data related to continuous marine fog stages from four series of geostationary meteorological satellites, along with meteorological observations and numerical analysis, covering 15 marine regions globally where maritime fog frequently occurs. Through pixel-level manual annotation by meteorological experts, we present the most comprehensive marine fog detection and forecasting dataset to date, named M4Fog, to bridge ocean and atmosphere. The dataset comprises 68,000 “super data cubes” along four dimensions: elements, latitude, longitude and time, with a temporal resolution of half an hour and a spatial resolution of 1 kilometer. Considering practical applications, we have defined and explored three meaningful tracks with multi-metric evaluation systems: static or dynamic marine fog detection, and spatio-temporal forecasting for cloud images. Extensive benchmarking and experiments demonstrate the rationality and effectiveness of the construction concept for proposed M4Fog. The data and codes are available to whole researchers through cloud platforms to develop ML-driven marine fog solutions and mitigate adverse impacts on human activities.\n","date":1718755200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718755200,"objectID":"9b5ebad71f44b86a67b9dd347931b000","permalink":"https://BUPT-PRIS-727.github.io/publications/m4fog/","publishdate":"2024-06-19T00:00:00Z","relpermalink":"/publications/m4fog/","section":"publications","summary":"Marine fog poses a significant hazard to global shipping, necessitating effective detection and forecasting to reduce economic losses. In recent years, several machine learning (ML) methods have demonstrated superior detection accuracy compared to traditional meteorological methods. However, most of these works are developed on proprietary datasets, and the few publicly accessible datasets are often limited to simplistic toy scenarios for research purposes. To advance the field, we have collected nearly a decade’s worth of multi-modal data related to continuous marine fog stages from four series of geostationary meteorological satellites, along with meteorological observations and numerical analysis, covering 15 marine regions globally where maritime fog frequently occurs. Through pixel-level manual annotation by meteorological experts, we present the most comprehensive marine fog detection and forecasting dataset to date, named M4Fog, to bridge ocean and atmosphere. The dataset comprises 68,000 “super data cubes” along four dimensions: elements, latitude, longitude and time, with a temporal resolution of half an hour and a spatial resolution of 1 kilometer. Considering practical applications, we have defined and explored three meaningful tracks with multi-metric evaluation systems: static or dynamic marine fog detection, and spatio-temporal forecasting for cloud images. Extensive benchmarking and experiments demonstrate the rationality and effectiveness of the construction concept for proposed M4Fog. The data and codes are available to whole researchers through cloud platforms to develop ML-driven marine fog solutions and mitigate adverse impacts on human activities.","tags":["Marine Fog","Dataset"],"title":"M4Fog: A Global Multi-Regional, Multi-Modal, and Multi-Stage Dataset for Marine Fog Detection and Forecasting to Bridge Ocean and Atmosphere","type":"publications"},{"authors":["Ziheng Yang","Ming Wu","Mengqiu Xu","Xun Zhu","Chuang Zhang","Bin Zhang"],"categories":null,"content":"Abstract Sea fog detection is a significant and challenging issue in meteorological satellite imagery. Distinguishing between sea fog and low clouds is challenging due to the similar morphology and brightness characteristics of these two phenomena on the imageries. Most of the existing deep learning methods are based on a single imagery feature extraction without the time-related features in imagery sequence. Although the designed temporal models, such as temporal U-Net, expand the available features from a single imagery to the consecutive frames and introduce general temporal information, the learned motion features are not explicit and can only be implicitly learned through a large amount of data. Thus, we introduce motion features obtained from continuous temporal imagery sequences into the sea fog detection task due to the discrepancy between sea fog and other types of clouds. In this article, under the motion features acquired by Horn–Schunck (HS) optical flow method and attention mechanisms, a Motion Attention Network (MoANet) for sea fog detection is proposed, named MoANet. We performed detailed experiments on the Himawaria-8 satellite imagery data set (H-8 Dataset). The Mean Intersection over Union (MIoU) of our method reaches 81.38%, which is 6.49% higher than the single imagery method. The visualization of the results shows that MoANet has more smooth edges, as well as detects more complete area than others. Furthermore, we validate on International Comprehensive Ocean-Atmosphere Data Set (ICOADS) through contrasting visibility value to prove the practicality of the proposed method and the accuracy achieves 90.65%.\n","date":1701993600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701993600,"objectID":"fb779f8b4ae0dc2a2c8f8a9e3dfbf623","permalink":"https://BUPT-PRIS-727.github.io/publications/moanet-a-motion-attention-network-for-sea-fog-detection-in-time-series-meteorological-satellite-imagery/","publishdate":"2023-12-08T00:00:00Z","relpermalink":"/publications/moanet-a-motion-attention-network-for-sea-fog-detection-in-time-series-meteorological-satellite-imagery/","section":"publications","summary":"Sea fog detection is a significant and challenging issue in meteorological satellite imagery. Distinguishing between sea fog and low clouds is challenging due to the similar morphology and brightness characteristics of these two phenomena on the imageries. Most of the existing deep learning methods are based on a single imagery feature extraction without the time-related features in imagery sequence. Although the designed temporal models, such as temporal U-Net, expand the available features from a single imagery to the consecutive frames and introduce general temporal information, the learned motion features are not explicit and can only be implicitly learned through a large amount of data. Thus, we introduce motion features obtained from continuous temporal imagery sequences into the sea fog detection task due to the discrepancy between sea fog and other types of clouds. In this article, under the motion features acquired by Horn–Schunck (HS) optical flow method and attention mechanisms, a Motion Attention Network (MoANet) for sea fog detection is proposed, named MoANet. We performed detailed experiments on the Himawaria-8 satellite imagery data set (H-8 Dataset). The Mean Intersection over Union (MIoU) of our method reaches 81.38%, which is 6.49% higher than the single imagery method. The visualization of the results shows that MoANet has more smooth edges, as well as detects more complete area than others. Furthermore, we validate on International Comprehensive Ocean-Atmosphere Data Set (ICOADS) through contrasting visibility value to prove the practicality of the proposed method and the accuracy achieves 90.65%.","tags":["Attention map","image segmentation","motion features","optical flow","sea fog detection"],"title":"MoANet: A Motion Attention Network for Sea Fog Detection in Time Series Meteorological Satellite Imagery","type":"publications"},{"authors":null,"categories":null,"content":"from IPython.core.display import Image Image(\u0026#39;https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png\u0026#39;) print(\u0026#34;Welcome to Academic!\u0026#34;) Welcome to Academic! Organize your notebooks Place the notebooks that you would like to publish in a notebooks folder at the root of your website.\nImport the notebooks into your site pipx install academic academic import \u0026#39;notebooks/**.ipynb\u0026#39; content/post/ --verbose The notebooks will be published to the folder you specify above. In this case, they will be published to your content/post/ folder.\n","date":1699056000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1699056000,"objectID":"94fa5e486d3bf3e0941e2ff6e7126c06","permalink":"https://BUPT-PRIS-727.github.io/post/blog-with-jupyter/","publishdate":"2023-11-04T00:00:00Z","relpermalink":"/post/blog-with-jupyter/","section":"post","summary":"Easily blog from Jupyter notebooks!","tags":null,"title":"Blog with Jupyter Notebooks!","type":"post"},{"authors":["Yixiang Huang","Ming Wu","Xin Jiang","Jiaao Li","Mengqiu Xu","Chuang Zhang","Jun Guo"],"categories":null,"content":"Abstract Sea fog detection is a challenging and significant task in the field of remote sensing. Deep learning-based methods have shown promising potential, but require a large amount of pixel-level labeled data that are time-consuming and labor-intensive to acquire. To scale up the dataset and overcome the limitations of pixel-level annotation, we attempt to explore the existing knowledge from historical statistics for label-efficient sea fog detection. In this article, we propose an image-level weakly supervised sea fog detection dataset (WS-SFDD) and a novel weakly supervised sea fog detection framework via prototype learning, named ProCAM. According to the sea fog events recorded by the Marine Weather Review published quarterly by the National Meteorological Center of China, we collect the sea fog images from Himawari-8 satellite data and obtain free image-level labels to construct the dataset. However, with image-level annotations, the existing weakly supervised semantic segmentation (WSSS) methods mainly rely on class activation maps (CAMs) and have limitations when applied to such a specific scenario: 1) the pseudo-labels (PLs) mainly cover the most discriminative part of object regions that are incomplete; 2) the background is complex with varying atmospheric conditions, and it is difficult to distinguish sea fog from low clouds due to their high similarity in spectral characteristics; and 3) the co-occurring context, such as “sea,” distracts the model and thus degrades the performance. To address the above issues, in our proposed ProCAM, we first design a prototype reactivation (PRA) module that reactivates self-similar sea fog regions by pixel-to-prototype feature matching to improve the robustness and completeness of CAMs. Then, we develop a pixel-to-prototype contrastive (PPC) learning method to increase the distance between sea fog and background in the embedding space for learning more discriminative dense features. Finally, a self-augmented regularization (SAR) strategy is presented to decouple sea fog from its co-ocurring context, and thus avoid background interference. Extensive experiments on the WS-SFDD dataset demonstrate that our proposed method ProCAM achieves superior performance with an $F1$ score of 77.59% and a critical success index (CSI) of 63.39%. To the best of our knowledge, this is the first work to perform image-level weakly supervised sea fog detection in remote sensing images. The dataset and code are available at https://github.com/yixianghuang/ProCAM\n","date":1696982400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696982400,"objectID":"d4aceb76ac53ebbc2476d8c12fb65a36","permalink":"https://BUPT-PRIS-727.github.io/publications/weakly-supervised-sea-fog-detection-in-remote-sensing-images-via-prototype-learning/","publishdate":"2023-10-11T00:00:00Z","relpermalink":"/publications/weakly-supervised-sea-fog-detection-in-remote-sensing-images-via-prototype-learning/","section":"publications","summary":"Sea fog detection is a challenging and significant task in the field of remote sensing. Deep learning-based methods have shown promising potential, but require a large amount of pixel-level labeled data that are time-consuming and labor-intensive to acquire. To scale up the dataset and overcome the limitations of pixel-level annotation, we attempt to explore the existing knowledge from historical statistics for label-efficient sea fog detection. In this article, we propose an image-level weakly supervised sea fog detection dataset (WS-SFDD) and a novel weakly supervised sea fog detection framework via prototype learning, named ProCAM. According to the sea fog events recorded by the Marine Weather Review published quarterly by the National Meteorological Center of China, we collect the sea fog images from Himawari-8 satellite data and obtain free image-level labels to construct the dataset. However, with image-level annotations, the existing weakly supervised semantic segmentation (WSSS) methods mainly rely on class activation maps (CAMs) and have limitations when applied to such a specific scenario: 1) the pseudo-labels (PLs) mainly cover the most discriminative part of object regions that are incomplete; 2) the background is complex with varying atmospheric conditions, and it is difficult to distinguish sea fog from low clouds due to their high similarity in spectral characteristics; and 3) the co-occurring context, such as “sea,” distracts the model and thus degrades the performance. To address the above issues, in our proposed ProCAM, we first design a prototype reactivation (PRA) module that reactivates self-similar sea fog regions by pixel-to-prototype feature matching to improve the robustness and completeness of CAMs. Then, we develop a pixel-to-prototype contrastive (PPC) learning method to increase the distance between sea fog and background in the embedding space for learning more discriminative dense features. Finally, a self-augmented regularization (SAR) strategy is presented to decouple sea fog from its co-ocurring context, and thus avoid background interference. Extensive experiments on the WS-SFDD dataset demonstrate that our proposed method ProCAM achieves superior performance with an $F1$ score of 77.59% and a critical success index (CSI) of 63.39%. To the best of our knowledge, this is the first work to perform image-level weakly supervised sea fog detection in remote sensing images. The dataset and code are available at https://github.com/yixianghuang/ProCAM","tags":["Prototype learning","remote sensing image segmentation","sea fog detection","weakly supervised learning","Dataset"],"title":"Weakly Supervised Sea Fog Detection in Remote Sensing Images via Prototype Learning","type":"publications"},{"authors":["Haotian Yan","Sundingkai Su","Ming Wu","Mengqiu Xu","Yihao Zuo","Chuang Zhang","Bin Huang"],"categories":null,"content":"Abstract Sea fog detection (SFD) presents a significant challenge in the field of intelligent Earth observation, particularly in analyzing meteorological satellite imagery. Akin to various vision tasks, ImageNet pre-training is commonly used for pre-training SFD. However, in the context of multi-spectral meteorological satellite imagery, the initial step of deep learning has received limited attention. Recently, pre-training with Very High-Resolution (VHR) satellite imagery has gained increased popularity in remote-sensing vision tasks, showing the potential to replace ImageNet pre-training. However, it is worth noting that the meteorological satellite imagery applied in SFD, despite being an application of computer vision in remote sensing, differs greatly from VHR satellite imagery. To address the limitation of pre-training for SFD, this paper introduces a novel deep-learning paradigm to the meteorological domain driven by Masked Image Modeling (MIM). Our research reveals two key insights: (1) Pre-training with meteorological satellite imagery yields superior SFD performance compared to pre-training with nature imagery and VHR satellite imagery. (2) Incorporating the architectural characteristics of SFD models into a vanilla masked autoencoder (MAE) can augment the effectiveness of meteorological pre-training. To facilitate this research, we curate a pre-training dataset comprising 514,655 temporal multi-spectral meteorological satellite images, covering the Bohai Sea and Yellow Sea regions, which have the most sea fog occurrence. The longitude ranges from 115.00E to 128.75E, and the latitude ranges from 27.60N to 41.35N. Moreover, we introduce SeaMAE, a novel MAE that utilizes a Vision Transformer as the encoder and a convolutional hierarchical decoder, to learn meteorological representations. SeaMAE is pre-trained on this dataset and fine-tuned for SFD, resulting in state-of-the-art performance. For instance, using the ViT-Base as the backbone, SeaMAE pre-training which achieves 64.18% surpasses from-scratch learning, natural imagery pre-training, and VRH satellite imagery pre-training by 5.53% , 2.49% , and 2.21% , respectively, in terms of Intersection over Union of SFD.\n","date":1692576000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692576000,"objectID":"165669e84e52ba138b1a2ba41d82cb4c","permalink":"https://BUPT-PRIS-727.github.io/publications/seamae/","publishdate":"2023-08-21T00:00:00Z","relpermalink":"/publications/seamae/","section":"publications","summary":"Sea fog detection (SFD) presents a significant challenge in the field of intelligent Earth observation, particularly in analyzing meteorological satellite imagery. Akin to various vision tasks, ImageNet pre-training is commonly used for pre-training SFD. However, in the context of multi-spectral meteorological satellite imagery, the initial step of deep learning has received limited attention. Recently, pre-training with Very High-Resolution (VHR) satellite imagery has gained increased popularity in remote-sensing vision tasks, showing the potential to replace ImageNet pre-training. However, it is worth noting that the meteorological satellite imagery applied in SFD, despite being an application of computer vision in remote sensing, differs greatly from VHR satellite imagery. To address the limitation of pre-training for SFD, this paper introduces a novel deep-learning paradigm to the meteorological domain driven by Masked Image Modeling (MIM). Our research reveals two key insights: (1) Pre-training with meteorological satellite imagery yields superior SFD performance compared to pre-training with nature imagery and VHR satellite imagery. (2) Incorporating the architectural characteristics of SFD models into a vanilla masked autoencoder (MAE) can augment the effectiveness of meteorological pre-training. To facilitate this research, we curate a pre-training dataset comprising 514,655 temporal multi-spectral meteorological satellite images, covering the Bohai Sea and Yellow Sea regions, which have the most sea fog occurrence. The longitude ranges from 115.00E to 128.75E, and the latitude ranges from 27.60N to 41.35N. Moreover, we introduce SeaMAE, a novel MAE that utilizes a Vision Transformer as the encoder and a convolutional hierarchical decoder, to learn meteorological representations. SeaMAE is pre-trained on this dataset and fine-tuned for SFD, resulting in state-of-the-art performance. For instance, using the ViT-Base as the backbone, SeaMAE pre-training which achieves 64.18% surpasses from-scratch learning, natural imagery pre-training, and VRH satellite imagery pre-training by 5.53% , 2.49% , and 2.21% , respectively, in terms of Intersection over Union of SFD.","tags":["sea fog detection","pre-training","masked autoencoders","meteorological satellite imagery","Dataset"],"title":"SeaMAE: Masked Pre-Training with Meteorological Satellite Imagery for Sea Fog Detection","type":"publications"},{"authors":["Xun Zhu","Yutong Xiong","Ming Wu","Gaozhen Nie","Bin Zhang","Ziheng Yang"],"categories":null,"content":"Abstract Weather forecasting is one of the cornerstones of meteorological work. In this paper, we present a new benchmark dataset named Weather2K, which aims to make up for the deficiencies of existing weather forecasting datasets in terms of real-time, reliability, and diversity, as well as the key bottleneck of data quality. To be specific, our Weather2K is featured from the following aspects: 1) Reliable and real-time data. The data is hourly collected from 2,130 ground weather stations covering an area of 6 million square kilometers. 2) Multivariate meteorological variables. 20 meteorological factors and 3 constants for position information are provided with a length of 40,896 time steps. 3) Applicable to diverse tasks. We conduct a set of baseline tests on time series forecasting and spatio-temporal forecasting. To the best of our knowledge, our Weather2K is the first attempt to tackle weather forecasting task by taking full advantage of the strengths of observation data from ground weather stations. Based on Weather2K, we further propose Meteorological Factors based Multi-Graph Convolution Network (MFMGCN), which can effectively construct the intrinsic correlation among geographic locations based on meteorological factors. Sufficient experiments show that MFMGCN improves both the forecasting performance and temporal robustness. We hope our Weather2K can significantly motivate researchers to develop efficient and accurate algorithms to advance the task of weather forecasting. The dataset can be available at https://github.com/bycnfz/weather2k/ .\n","date":1676937600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1676937600,"objectID":"1b39a215c1597903a690cd3c7a06a7cc","permalink":"https://BUPT-PRIS-727.github.io/publications/weather2k/","publishdate":"2023-02-21T00:00:00Z","relpermalink":"/publications/weather2k/","section":"publications","summary":"Weather forecasting is one of the cornerstones of meteorological work. In this paper, we present a new benchmark dataset named Weather2K, which aims to make up for the deficiencies of existing weather forecasting datasets in terms of real-time, reliability, and diversity, as well as the key bottleneck of data quality. To be specific, our Weather2K is featured from the following aspects: 1) Reliable and real-time data. The data is hourly collected from 2,130 ground weather stations covering an area of 6 million square kilometers. 2) Multivariate meteorological variables. 20 meteorological factors and 3 constants for position information are provided with a length of 40,896 time steps. 3) Applicable to diverse tasks. We conduct a set of baseline tests on time series forecasting and spatio-temporal forecasting. To the best of our knowledge, our Weather2K is the first attempt to tackle weather forecasting task by taking full advantage of the strengths of observation data from ground weather stations. Based on Weather2K, we further propose Meteorological Factors based Multi-Graph Convolution Network (MFMGCN), which can effectively construct the intrinsic correlation among geographic locations based on meteorological factors. Sufficient experiments show that MFMGCN improves both the forecasting performance and temporal robustness. We hope our Weather2K can significantly motivate researchers to develop efficient and accurate algorithms to advance the task of weather forecasting. The dataset can be available at https://github.com/bycnfz/weather2k/ .","tags":["Weather Forecasting","Dataset","Multi-Graph","Dataset"],"title":"Weather2K: A Multivariate Spatio-Temporal Benchmark Dataset for Meteorological Forecasting Based on Real-Time Observation Data from Ground Weather Stations","type":"publications"},{"authors":["Rui Min","Ming Wu","Mengqiu Xu","Xun Zhu"],"categories":null,"content":"Abstract Visibility prediction in coastal areas has always been an important issue affecting the safety of residents and the efficiency of urban transportation. The visibility prediction methods currently used by meteorological centers are mainly based on the statistical forecast with relatively low prediction accuracy and high computational complexity. These methods cannot work well with large amounts of data. However, with the rapid development of deep learning technology, the use of deep learning has become a primary trend. In this paper, we propose our visibility prediction model based on (Long Short-Term Memory) LSTM network and self-attention mechanism. The model takes Medium-range Forecasts Data from European Centre for Mediumrange Weather Forecasting (ECMWF) which we use EC data to refer it for simplicity and observatory visibility data as input to predict and uses the LSTM network as the backbone to extract time series information. We also use self-attention mechanism to process the input data before the data is input to the model to let the model better focus on the valuable information for prediction. Compared with the predicted visibility in EC data, our proposed method improved the 3-hour prediction accuracy by 20%, 1.5 times, and 8 times for high-range, medium-range, and low-range visibility, respectively. We also find the data imbalance will greatly affect the prediction accuracy for low-visibility data and use the weighted-loss and mix-up data augmentation strategy model in our model training. We improved the accuracy of low-visibility data by 1.2 times while the prediction results of high-visibility and medium-visibility data remained almost the same. In addition, we conduct several experiments to verify the effectiveness of our model design and the rationality of data augmentation.\n","date":1674086400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1674086400,"objectID":"b88013aa6ee8951cefd2cf5aaeaac338","permalink":"https://BUPT-PRIS-727.github.io/publications/attention-based-long-short-term-memory-network-for-coastal-visibility-forecast/","publishdate":"2023-01-19T00:00:00Z","relpermalink":"/publications/attention-based-long-short-term-memory-network-for-coastal-visibility-forecast/","section":"publications","summary":"Visibility prediction in coastal areas has always been an important issue affecting the safety of residents and the efficiency of urban transportation. The visibility prediction methods currently used by meteorological centers are mainly based on the statistical forecast with relatively low prediction accuracy and high computational complexity. These methods cannot work well with large amounts of data. However, with the rapid development of deep learning technology, the use of deep learning has become a primary trend. In this paper, we propose our visibility prediction model based on (Long Short-Term Memory) LSTM network and self-attention mechanism. The model takes Medium-range Forecasts Data from European Centre for Mediumrange Weather Forecasting (ECMWF) which we use EC data to refer it for simplicity and observatory visibility data as input to predict and uses the LSTM network as the backbone to extract time series information. We also use self-attention mechanism to process the input data before the data is input to the model to let the model better focus on the valuable information for prediction. Compared with the predicted visibility in EC data, our proposed method improved the 3-hour prediction accuracy by 20%, 1.5 times, and 8 times for high-range, medium-range, and low-range visibility, respectively. We also find the data imbalance will greatly affect the prediction accuracy for low-visibility data and use the weighted-loss and mix-up data augmentation strategy model in our model training. We improved the accuracy of low-visibility data by 1.2 times while the prediction results of high-visibility and medium-visibility data remained almost the same. In addition, we conduct several experiments to verify the effectiveness of our model design and the rationality of data augmentation.","tags":["Coastal visibility prediction","Deep learning","Long short-term memory","Self-attention","Data imbalance"],"title":"Attention based Long Short-Term Memory Network for Coastal Visibility Forecast","type":"publications"},{"authors":["Xun Zhu","Mengqiu Xu","Ming Wu","Chuang Zhang","Bin Zhang"],"categories":null,"content":"Abstract Sea fog recognition is a challenging and significant semantic segmentation task in remote sensing images. The fully supervised learning method relies on the pixel-level label, which is labor-intensive and time-consuming. Moreover, it is impossible to accurately annotate all pixels of the sea fog region due to the limited ability of the human eye to distinguish between low clouds and sea fog. In this paper, we propose a novel approach of point-based annotation for weakly supervised semantic segmentation with the auxiliary information of International Comprehensive Ocean-Atmosphere Data Set (ICOADS) visibility data. It only needs several definite points for both foreground and background, which significantly reduces the annotation cost of manpower. We conduct extensive experiments on Himawari-8 satellite remote sensing images to demonstrate the effectiveness of our annotation method. The mean intersection over union (mIoU) and overall recognition accuracy of our annotation method reach 82.72% and 95.18 %, respectively. Compared with the fully supervised learning method, the accuracy and the recognition rate of sea fog area are improved with a maximum increase of 7.69% and 9.69 %, respectively.\n","date":1673827200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673827200,"objectID":"2a9675c76e17e47a2cf380ebb0f9244b","permalink":"https://BUPT-PRIS-727.github.io/publications/annotating-only-at-definite-pixels-a-novel-weakly-supervised-semantic-segmentation-method-for-sea-fog-recognition/","publishdate":"2023-01-16T00:00:00Z","relpermalink":"/publications/annotating-only-at-definite-pixels-a-novel-weakly-supervised-semantic-segmentation-method-for-sea-fog-recognition/","section":"publications","summary":"Sea fog recognition is a challenging and significant semantic segmentation task in remote sensing images. The fully supervised learning method relies on the pixel-level label, which is labor-intensive and time-consuming. Moreover, it is impossible to accurately annotate all pixels of the sea fog region due to the limited ability of the human eye to distinguish between low clouds and sea fog. In this paper, we propose a novel approach of point-based annotation for weakly supervised semantic segmentation with the auxiliary information of International Comprehensive Ocean-Atmosphere Data Set (ICOADS) visibility data. It only needs several definite points for both foreground and background, which significantly reduces the annotation cost of manpower. We conduct extensive experiments on Himawari-8 satellite remote sensing images to demonstrate the effectiveness of our annotation method. The mean intersection over union (mIoU) and overall recognition accuracy of our annotation method reach 82.72% and 95.18 %, respectively. Compared with the fully supervised learning method, the accuracy and the recognition rate of sea fog area are improved with a maximum increase of 7.69% and 9.69 %, respectively.","tags":["sea fog recognition","weakly supervised learning","point annotation","semantic segmentation","remote sensing image"],"title":"Annotating Only at Definite Pixels- A Novel Weakly Supervised Semantic Segmentation Method for Sea Fog Recognition","type":"publications"},{"authors":["Sibo Wu","Mengqiu Xu","Ming Wu","Chuang Zhang","Hua Shen"],"categories":null,"content":"Abstract Remote sensing images serve a significant role in earth observation to tackle climate change and post-disaster reconstruction concerns. However, optical images are obscured by clouds or haze, preventing precise earth observation; hence, cloud removal has been a hot topic among concerned scholars. The objective of this article is to make cloud removal more efficient and explicable by proposing three principles: identifying clouds, guessing objects beneath the clouds, and reconstructing the cloudy area. In addition, a modified dual contrastive learning Generative Adversarial Network is proposed based on these three principles by adding cloud detection and weight sharing strategy to obtain cloud semantics. In particular, we align two datasets by forming a quaternary sample pair that includes not only optical pictures and SAR images, but also region information for a more precise reconstruction. Our experiment results on the integrated dataset reveal the superiority of proposed method over previous cloud removal methods and the effectiveness of added modules through ablation experiments, with PSNR and SSIM values of 26.2 and 0.728, respectively.\n","date":1673827200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673827200,"objectID":"56f7bb159c6abd7623668278820a13b2","permalink":"https://BUPT-PRIS-727.github.io/publications/identify-guess-and-reconstruct-three-principles-for-cloud-removal-task/","publishdate":"2023-01-16T00:00:00Z","relpermalink":"/publications/identify-guess-and-reconstruct-three-principles-for-cloud-removal-task/","section":"publications","summary":"Remote sensing images serve a significant role in earth observation to tackle climate change and post-disaster reconstruction concerns. However, optical images are obscured by clouds or haze, preventing precise earth observation; hence, cloud removal has been a hot topic among concerned scholars. The objective of this article is to make cloud removal more efficient and explicable by proposing three principles: identifying clouds, guessing objects beneath the clouds, and reconstructing the cloudy area. In addition, a modified dual contrastive learning Generative Adversarial Network is proposed based on these three principles by adding cloud detection and weight sharing strategy to obtain cloud semantics. In particular, we align two datasets by forming a quaternary sample pair that includes not only optical pictures and SAR images, but also region information for a more precise reconstruction. Our experiment results on the integrated dataset reveal the superiority of proposed method over previous cloud removal methods and the effectiveness of added modules through ablation experiments, with PSNR and SSIM values of 26.2 and 0.728, respectively.","tags":["Cloud Removal","Dual Contrastive Learning GAN","Remote Sensing","Deep Learning"],"title":"Identify, Guess and Reconstruct- Three Principles for Cloud Removal Task","type":"publications"},{"authors":["Mengqiu Xu","Ming Wu","Kaixin Chen","Chuang Zhang","Jun Guo"],"categories":null,"content":"Abstract With the rapid development of the remote sensing monitoring and computer vision technology, the deep learning method has made a great progress to achieve applications such as earth observation, climate change and even space exploration. However, the model trained on existing data cannot be directly used to handle the new remote sensing data, and labeling the new data is also time-consuming and labor-intensive. Unsupervised Domain Adaptation (UDA) is one of the solutions to the aforementioned problems of labeled data defined as the source domain and unlabeled data as the target domain, i.e., its essential purpose is to obtain a well-trained model and tackle the problem of data distribution discrepancy defined as the domain shift between the source and target domain. There are a lot of reviews that have elaborated on UDA methods based on natural data, but few of these studies take into consideration thorough remote sensing applications and contributions. Thus, in this paper, in order to explore the further progress and development of UDA methods in remote sensing, based on the analysis of the causes of domain shift, a comprehensive review is provided with a fine-grained taxonomy of UDA methods applied for remote sensing data, which includes Generative training, Adversarial training, Self-training and Hybrid training methods, to better assist scholars in understanding remote sensing data and further advance the development of methods. Moreover, remote sensing applications are introduced by a thorough dataset analysis. Meanwhile, we sort out definitions and methodology introductions of partial, open-set and multi-domain UDA, which are more pertinent to real-world remote sensing applications. We can draw the conclusion that UDA methods in the field of remote sensing data are carried out later than those applied in natural images, and due to the domain gap caused by appearance differences, most of methods focus on how to use generative training (GT) methods to improve the model’s performance. Finally, we describe the potential deficiencies and further in-depth insights of UDA in the field of remote sensing.\n","date":1662163200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1662163200,"objectID":"e4e96f62eea238d511fa713f7516910e","permalink":"https://BUPT-PRIS-727.github.io/publications/the-eyes-of-the-gods/","publishdate":"2022-09-03T00:00:00Z","relpermalink":"/publications/the-eyes-of-the-gods/","section":"publications","summary":"With the rapid development of the remote sensing monitoring and computer vision technology, the deep learning method has made a great progress to achieve applications such as earth observation, climate change and even space exploration. However, the model trained on existing data cannot be directly used to handle the new remote sensing data, and labeling the new data is also time-consuming and labor-intensive. Unsupervised Domain Adaptation (UDA) is one of the solutions to the aforementioned problems of labeled data defined as the source domain and unlabeled data as the target domain, i.e., its essential purpose is to obtain a well-trained model and tackle the problem of data distribution discrepancy defined as the domain shift between the source and target domain. There are a lot of reviews that have elaborated on UDA methods based on natural data, but few of these studies take into consideration thorough remote sensing applications and contributions. Thus, in this paper, in order to explore the further progress and development of UDA methods in remote sensing, based on the analysis of the causes of domain shift, a comprehensive review is provided with a fine-grained taxonomy of UDA methods applied for remote sensing data, which includes Generative training, Adversarial training, Self-training and Hybrid training methods, to better assist scholars in understanding remote sensing data and further advance the development of methods. Moreover, remote sensing applications are introduced by a thorough dataset analysis. Meanwhile, we sort out definitions and methodology introductions of partial, open-set and multi-domain UDA, which are more pertinent to real-world remote sensing applications. We can draw the conclusion that UDA methods in the field of remote sensing data are carried out later than those applied in natural images, and due to the domain gap caused by appearance differences, most of methods focus on how to use generative training (GT) methods to improve the model’s performance. Finally, we describe the potential deficiencies and further in-depth insights of UDA in the field of remote sensing.","tags":["unsupervised domain adaptation","remote sensing data","a survey","deep learning"],"title":"The Eyes of the Gods: A Survey of Unsupervised Domain Adaptation Methods Based on Remote Sensing Data","type":"publications"},{"authors":["Bin Huang","Luming Xiao","Wen Feng","Mengqiu Xu","Ming Wu","Xiang Fang"],"categories":null,"content":"Abstract Meteorological satellites have become an indispensable meteorological tool for earth observation, as aiding in areas such as cloud detection, which has important guiding significance for maritime activities. However, it is time-consuming and labor-intensive to obtain fine-grained annotations provided by artificial experience or mature satellite cloud products for multi-spectral maritime cloud imageries, especially when new satellites are launched. Moreover, due to the data discrepancy caused by different detection bands, existing models have inadequate generalization performance compared to new satellites, and some cannot be directly migrated. In this paper, to reduce the data distribution’s discrepancy, an approach is presented based on unsupervised domain adaption method for marine cloud detection task based on Himawari-8 satellite data as a source domain and Fengyun-4 satellite data as a target domain. The goal of the proposed method is to leverage the representation power of adversarial learning to extract domain-invariant features, consisting of a segmentation model, a feature extract model for target domain, and a domain discriminator. In addition, aiming to remedy the discrepancy of detection bands, a band mapping module is designed to implement consistency between different bands. The result of the experiments demonstrated the effectiveness of the proposed method with a 7% improvement compared with the comparative experiment. We also designed a series of statistical experiments on different satellite data to further study cloudy perception representation, including data visualization experiment and cloud type statistics.\n","date":1660262400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1660262400,"objectID":"050d6c32cd0314f3e896871319895393","permalink":"https://BUPT-PRIS-727.github.io/publications/domain-adaptation-on-multiple-cloud-recognition-from-different-types-of-meteorological-satellite/","publishdate":"2022-08-12T00:00:00Z","relpermalink":"/publications/domain-adaptation-on-multiple-cloud-recognition-from-different-types-of-meteorological-satellite/","section":"publications","summary":"Meteorological satellites have become an indispensable meteorological tool for earth observation, as aiding in areas such as cloud detection, which has important guiding significance for maritime activities. However, it is time-consuming and labor-intensive to obtain fine-grained annotations provided by artificial experience or mature satellite cloud products for multi-spectral maritime cloud imageries, especially when new satellites are launched. Moreover, due to the data discrepancy caused by different detection bands, existing models have inadequate generalization performance compared to new satellites, and some cannot be directly migrated. In this paper, to reduce the data distribution’s discrepancy, an approach is presented based on unsupervised domain adaption method for marine cloud detection task based on Himawari-8 satellite data as a source domain and Fengyun-4 satellite data as a target domain. The goal of the proposed method is to leverage the representation power of adversarial learning to extract domain-invariant features, consisting of a segmentation model, a feature extract model for target domain, and a domain discriminator. In addition, aiming to remedy the discrepancy of detection bands, a band mapping module is designed to implement consistency between different bands. The result of the experiments demonstrated the effectiveness of the proposed method with a 7% improvement compared with the comparative experiment. We also designed a series of statistical experiments on different satellite data to further study cloudy perception representation, including data visualization experiment and cloud type statistics.","tags":["marine cloud classification","unsupervised domain adaptation","deep learning","transfer learning","semantic segmentation"],"title":"Domain Adaptation on Multiple Cloud Recognition From Different Types of Meteorological Satellite","type":"publications"},{"authors":["Mengqiu Xu","Ming Wu","Jun Guo","Chuang Zhang","Yubo Wang","Zhangyu Ma"],"categories":null,"content":"Abstract Sea fog detection with remote sensing images is a challenging task. Driven by the different image characteristics between fog and other types of clouds, such as textures and colors, it can be achieved by using image processing methods. Currently, most of the available methods are data-driven and relying on manual annotations. However, because few meteorological observations and buoys over the sea can be realized, obtaining visibility information to help the annotations is difficult. Considering the feasibility of obtaining abundant visible information over the land and the similarity between land fog and sea fog, we propose an unsupervised domain adaptation method to bridge the abundant labeled land fog data and the unlabeled sea fog data to realize the sea fog detection. We used a seeded region growing module to obtain pixel-level masks from rough-labels generated by the unsupervised domain adaptation model. Experimental results demonstrate that our proposed method achieves an accuracy of sea fog recognition up to 99.17%, which is nearly 3% higher than those vanilla methods.\n","date":1648771200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648771200,"objectID":"fc87a7574f8b1c7e895ec1a38d0d93c0","permalink":"https://BUPT-PRIS-727.github.io/publications/sea-fog-detection-based-on-unsupervised-domain-adaptation/","publishdate":"2022-04-01T00:00:00Z","relpermalink":"/publications/sea-fog-detection-based-on-unsupervised-domain-adaptation/","section":"publications","summary":"Sea fog detection with remote sensing images is a challenging task. Driven by the different image characteristics between fog and other types of clouds, such as textures and colors, it can be achieved by using image processing methods. Currently, most of the available methods are data-driven and relying on manual annotations. However, because few meteorological observations and buoys over the sea can be realized, obtaining visibility information to help the annotations is difficult. Considering the feasibility of obtaining abundant visible information over the land and the similarity between land fog and sea fog, we propose an unsupervised domain adaptation method to bridge the abundant labeled land fog data and the unlabeled sea fog data to realize the sea fog detection. We used a seeded region growing module to obtain pixel-level masks from rough-labels generated by the unsupervised domain adaptation model. Experimental results demonstrate that our proposed method achieves an accuracy of sea fog recognition up to 99.17%, which is nearly 3% higher than those vanilla methods.","tags":["Deep learning","Sea fog detection","Seeded region growing","Transfer learning","Unsupervised domain","adaptation"],"title":"Sea fog detection based on unsupervised domain adaptation","type":"publications"},{"authors":["Bin Huang","Ming Wu","Shuyue Sun","Wei Zhao","Zhanbei Cui","Cheng Lv"],"categories":null,"content":"Abstract Sea fog, whether on the sea or the coast, has adverse effects on transportation, marine fishing, marine development projects, and military activities due to its poor visibility. Therefore, realtime monitoring and forecasting of sea fog are essential. This paper proposes a multichannel image fusion segmentation algorithm for stationary meteorological satellites based on deep learning. The DLinkNet deep neural network semantic segmentation algorithm model is used to study the 16channel Himawari8 satellite data with a spatial resolution of 0.5 km in the Yellow Sea and the Bohai Sea. Using mIOU (mean Intersection Over Union) and observation value test as evaluation indicators, the mIOU on the test set is 0.9436, and comparing the results of satellite test data with the results of marine observation data. It was concluded that the accuracy rate of fog area (detect fog and real fog / detect fog) is 66.5%, the recognition rate of fog area (detect fog and real fog / (real fog-cloud coverage)) is 51.9%, and the detection accuracy rate (detection correct samples / total samples) is 93.2%. In conclusion, the method proposed in this paper can provide a reliable reference for sea fog monitoring.\n","date":1640908800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640908800,"objectID":"75ce88b2e0dac211ce3c8885dcb30c3e","permalink":"https://BUPT-PRIS-727.github.io/publications/sea-fog-monitoring-method-based-on-deep-learning-satellite-multi-channel-image-fusion/","publishdate":"2021-12-31T00:00:00Z","relpermalink":"/publications/sea-fog-monitoring-method-based-on-deep-learning-satellite-multi-channel-image-fusion/","section":"publications","summary":"Sea fog, whether on the sea or the coast, has adverse effects on transportation, marine fishing, marine development projects, and military activities due to its poor visibility. Therefore, realtime monitoring and forecasting of sea fog are essential. This paper proposes a multichannel image fusion segmentation algorithm for stationary meteorological satellites based on deep learning. The DLinkNet deep neural network semantic segmentation algorithm model is used to study the 16channel Himawari8 satellite data with a spatial resolution of 0.5 km in the Yellow Sea and the Bohai Sea. Using mIOU (mean Intersection Over Union) and observation value test as evaluation indicators, the mIOU on the test set is 0.9436, and comparing the results of satellite test data with the results of marine observation data. It was concluded that the accuracy rate of fog area (detect fog and real fog / detect fog) is 66.5%, the recognition rate of fog area (detect fog and real fog / (real fog-cloud coverage)) is 51.9%, and the detection accuracy rate (detection correct samples / total samples) is 93.2%. In conclusion, the method proposed in this paper can provide a reliable reference for sea fog monitoring.","tags":["sea fog detection","multi-channel image fusion","D-LinkNet","Himawari-8"],"title":"Sea Fog Monitoring Method Based on Deep Learning Satellite Multi-channel Image Fusion","type":"publications"},{"authors":["Yixiang Huang","Ming Wu","Jun Guo","Chuang Zhang","Mengqiu Xu"],"categories":null,"content":"Abstract Sea fog detection is a challenging and essential issue in satellite remote sensing. Although conventional threshold methods and deep learning methods can achieve pixel-level classification, it is difficult to distinguish ambiguous boundaries and thin structures from the background. Considering the correlations between neighbor pixels and the affinities between superpixels, a correlation context-driven method for sea fog detection is proposed in this letter, which mainly consists of a two-stage superpixel-based fully convolutional network (SFCNet), named SFCNet. A fully connected Conditional Random Field (CRF) is utilized to model the dependencies between pixels. To alleviate the problem of high cloud occlusion, an attentive Generative Adversarial Network (GAN) is implemented for image enhancement by exploiting contextual information. Experimental results demonstrate that our proposed method achieves 91.65% mIoU and obtains more refined segmentation results, performing well in detecting fogs in small, broken bits and weak contrast thin structures, as well as detects more obscured parts.\n","date":1627257600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627257600,"objectID":"a673dae4b48d3be4c6b83dcd48761bf1","permalink":"https://BUPT-PRIS-727.github.io/publications/a-correlation-context-driven-method-for-sea-fog-detection-in-meteorological-satellite-imagery/","publishdate":"2021-07-26T00:00:00Z","relpermalink":"/publications/a-correlation-context-driven-method-for-sea-fog-detection-in-meteorological-satellite-imagery/","section":"publications","summary":"Sea fog detection is a challenging and essential issue in satellite remote sensing. Although conventional threshold methods and deep learning methods can achieve pixel-level classification, it is difficult to distinguish ambiguous boundaries and thin structures from the background. Considering the correlations between neighbor pixels and the affinities between superpixels, a correlation context-driven method for sea fog detection is proposed in this letter, which mainly consists of a two-stage superpixel-based fully convolutional network (SFCNet), named SFCNet. A fully connected Conditional Random Field (CRF) is utilized to model the dependencies between pixels. To alleviate the problem of high cloud occlusion, an attentive Generative Adversarial Network (GAN) is implemented for image enhancement by exploiting contextual information. Experimental results demonstrate that our proposed method achieves 91.65% mIoU and obtains more refined segmentation results, performing well in detecting fogs in small, broken bits and weak contrast thin structures, as well as detects more obscured parts.","tags":["Deep learning","satellite imagery","sea fog detection","superpixel"],"title":"A Correlation Context-Driven Method for Sea Fog Detection in Meteorological Satellite Imagery","type":"publications"},{"authors":["admin","吳恩達"],"categories":["Demo","教程"],"content":"import libr print(\u0026#39;hello\u0026#39;) Overview The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site The template can be modified and customised to suit your needs. It’s a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a no-code solution (write in Markdown and customize with YAML parameters) and having flexibility to later add even deeper personalization with HTML and CSS You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more Get Started 👉 Create a new site 📚 Personalize your site 💬 Chat with the Wowchemy community or Hugo community 🐦 Twitter: @wowchemy @GeorgeCushen #MadeWithWowchemy 💡 Request a feature or report a bug for Wowchemy ⬆️ Updating Wowchemy? View the Update Tutorial and Release Notes Crowd-funded open-source software To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.\n❤️ Click here to become a sponsor and help support Wowchemy’s future ❤️ As a token of appreciation for sponsoring, you can unlock these awesome rewards and extra features 🦄✨\nEcosystem Hugo Academic CLI: Automatically import publications from BibTeX Inspiration Check out the latest demo of what you’ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\nFeatures Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 34+ language packs including English, 中文, and Português Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files. Themes Wowchemy and its templates come with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\nChoose a stunning theme and font for your site. Themes are fully customizable.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1607817600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607817600,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"https://BUPT-PRIS-727.github.io/post/getting-started/","publishdate":"2020-12-13T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Welcome 👋 We know that first impressions are important, so we've populated your new site with some initial content to help you get familiar with everything in no time.","tags":["Academic","开源"],"title":"Welcome to Hugo Blox Builder, the website builder for Hugo","type":"post"},{"authors":null,"categories":null,"content":"Hugo Blox Builder is designed to give technical content creators a seamless experience. You can focus on the content and Wowchemy handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.\nOn this page, you’ll find some examples of the types of technical content that can be rendered with Wowchemy.\nExamples Code Wowchemy supports a Markdown extension for highlighting code syntax. You can customize the styles under the syntax_highlighter option in your config/_default/params.yaml file.\n```python import pandas as pd data = pd.read_csv(\u0026#34;data.csv\u0026#34;) data.head() ``` renders as\nimport pandas as pd data = pd.read_csv(\u0026#34;data.csv\u0026#34;) data.head() Mindmaps Wowchemy supports a Markdown extension for mindmaps.\nSimply insert a Markdown markmap code block and optionally set the height of the mindmap as shown in the example below.\nA simple mindmap defined as a Markdown list:\n```markmap {height=\u0026#34;200px\u0026#34;} - Hugo Modules - wowchemy - blox-plugins-netlify - blox-plugins-netlify-cms - blox-plugins-reveal ``` renders as\n- Hugo Modules - wowchemy - blox-plugins-netlify - blox-plugins-netlify-cms - blox-plugins-reveal A more advanced mindmap with formatting, code blocks, and math:\n```markmap - Mindmaps - Links - [Wowchemy Docs](https://docs.hugoblox.com/) - [Discord Community](https://discord.gg/z8wNYzb) - [GitHub](https://github.com/HugoBlox/hugo-blox-builder) - Features - Markdown formatting - **inline** ~~text~~ *styles* - multiline text - `inline code` - ```js console.log(\u0026#39;hello\u0026#39;); console.log(\u0026#39;code block\u0026#39;); ``` - Math: $x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}$ ``` renders as\n- Mindmaps - Links - [Wowchemy Docs](https://docs.hugoblox.com/) - [Discord Community](https://discord.gg/z8wNYzb) - [GitHub](https://github.com/HugoBlox/hugo-blox-builder) - Features - Markdown formatting - **inline** ~~text~~ *styles* - multiline text - `inline code` - ```js console.log(\u0026#39;hello\u0026#39;); console.log(\u0026#39;code block\u0026#39;); ``` - Math: $x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}$ Charts Wowchemy supports the popular Plotly format for interactive charts.\nSave your Plotly JSON in your page folder, for example line-chart.json, and then add the {{\u0026lt; chart data=\u0026#34;line-chart\u0026#34; \u0026gt;}} shortcode where you would like the chart to appear.\nDemo:\nYou might also find the Plotly JSON Editor useful.\nMath Wowchemy supports a Markdown extension for $\\LaTeX$ math. You can enable this feature by toggling the math option in your config/_default/params.yaml file.\nTo render inline or block math, wrap your LaTeX math with {{\u0026lt; math \u0026gt;}}$...${{\u0026lt; /math \u0026gt;}} or {{\u0026lt; math \u0026gt;}}$$...$${{\u0026lt; /math \u0026gt;}}, respectively. (We wrap the LaTeX math in the Wowchemy math shortcode to prevent Hugo rendering our math as Markdown. The math shortcode is new in v5.5-dev.)\nExample math block:\n{{\u0026lt; math \u0026gt;}} $$ \\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |}{\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2} $$ {{\u0026lt; /math \u0026gt;}} renders as\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |}{\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2}$$ Example inline math {{\u0026lt; math \u0026gt;}}$\\nabla F(\\mathbf{x}_{n})${{\u0026lt; /math \u0026gt;}} renders as $\\nabla F(\\mathbf{x}_{n})$.\nExample multi-line math using the math linebreak (\\\\):\n{{\u0026lt; math \u0026gt;}} $$f(k;p_{0}^{*}) = \\begin{cases}p_{0}^{*} \u0026amp; \\text{if }k=1, \\\\ 1-p_{0}^{*} \u0026amp; \\text{if }k=0.\\end{cases}$$ {{\u0026lt; /math \u0026gt;}} renders as\n$$ f(k;p_{0}^{*}) = \\begin{cases}p_{0}^{*} \u0026amp; \\text{if }k=1, \\\\ 1-p_{0}^{*} \u0026amp; \\text{if }k=0.\\end{cases} $$ Diagrams Wowchemy supports a Markdown extension for diagrams. You can enable this feature by toggling the diagram option in your config/_default/params.toml file or by adding diagram: true to your page front matter.\nAn example flowchart:\n```mermaid graph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2] ``` renders as\ngraph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2] An example sequence diagram:\n```mermaid sequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good! ``` renders as\nsequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good! An example Gantt diagram:\n```mermaid gantt section Section Completed :done, des1, 2014-01-06,2014-01-08 Active :active, des2, 2014-01-07, 3d Parallel 1 : des3, after des1, 1d Parallel 2 : des4, after des1, 1d Parallel 3 : des5, after des3, 1d Parallel 4 : des6, after des4, 1d ``` renders as\ngantt section …","date":1562889600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562889600,"objectID":"07e02bccc368a192a0c76c44918396c3","permalink":"https://BUPT-PRIS-727.github.io/post/writing-technical-content/","publishdate":"2019-07-12T00:00:00Z","relpermalink":"/post/writing-technical-content/","section":"post","summary":"Hugo Blox Builder is designed to give technical content creators a seamless experience. You can focus on the content and Wowchemy handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.","tags":null,"title":"Writing technical content in Markdown","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Hugo Blox Builder Hugo Blox Builder | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://BUPT-PRIS-727.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Hugo Blox Builder's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"82c8b6050a311dac448568d7d05fa523","permalink":"https://BUPT-PRIS-727.github.io/project/%E5%9F%BA%E4%BA%8E%E6%97%A0%E7%9B%91%E7%9D%A3%E5%9F%9F%E9%80%82%E5%BA%94%E7%9A%84%E4%B8%8D%E5%90%8C%E7%B1%BB%E5%9E%8B%E6%B0%94%E8%B1%A1%E5%8D%AB%E6%98%9F%E7%9A%84%E5%A4%9A%E4%BA%91%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/%E5%9F%BA%E4%BA%8E%E6%97%A0%E7%9B%91%E7%9D%A3%E5%9F%9F%E9%80%82%E5%BA%94%E7%9A%84%E4%B8%8D%E5%90%8C%E7%B1%BB%E5%9E%8B%E6%B0%94%E8%B1%A1%E5%8D%AB%E6%98%9F%E7%9A%84%E5%A4%9A%E4%BA%91%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F/","section":"project","summary":"","tags":null,"title":"基于无监督域适应的不同类型气象卫星的多云识别系统","type":"project"}]